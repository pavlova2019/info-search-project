{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/st079069/dataset_for_faiss/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_base_path = data_path + \"chunks_embeddings_jina_base.json\"\n",
    "chunks_pass_path = data_path + \"chunks_embeddings_jina_passage.json\"\n",
    "\n",
    "questions_base_path = data_path + \"questions_embeddings_jina_base.npy\"\n",
    "questions_query_path = data_path + \"questions_embeddings_jina_query.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_base = pd.read_json(chunks_base_path, orient='index')\n",
    "\n",
    "chunks_pass = pd.read_json(chunks_pass_path, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_base_emb = np.load(questions_base_path)\n",
    "\n",
    "questions_query_emb = np.load(questions_query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9056cda74a491b9dfb0dbc8b84a2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c61b76eeb546cfa380679ec33418c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks_base_ids = dict()\n",
    "\n",
    "i = 0\n",
    "for id, emb_list in tqdm(chunks_base.iterrows()):\n",
    "    ln = len(emb_list.dropna())\n",
    "    chunks_base_ids[id] = np.arange(i, i+ln)\n",
    "    i += ln\n",
    "\n",
    "chunks_base_emb = np.array([emb for emb in tqdm(chunks_base.values.flatten()) if emb is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b31b517efb4432697ce5c0389d5449c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4af820ef91e4f3d8a5d03470c604ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks_pass_ids = dict()\n",
    "\n",
    "i = 0\n",
    "for id, emb_list in tqdm(chunks_pass.iterrows()):\n",
    "    ln = len(emb_list.dropna())\n",
    "    chunks_pass_ids[id] = np.arange(i, i+ln)\n",
    "    i += ln\n",
    "\n",
    "chunks_pass_emb = np.array([emb for emb in tqdm(chunks_pass.values.flatten()) if emb is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_w_params(name: str, params: list[object]):\n",
    "    return \"_\".join([name] + [p.__name__ if isinstance(p, faiss.Index) else str(p) for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = chunks_pass_emb.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params = {\"flat\": {\"flat\" : flat_index}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW paremeters\n",
    "M_list = [16, 32, 64]  # number of connections each vertex will have\n",
    "ef_search_list = [16, 32, 64]  # depth of layers explored during search\n",
    "ef_construction_list = [16, 32, 64]  # depth of layers explored during index construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"hnsw\"] = dict()\n",
    "\n",
    "for params in product(*[M_list, ef_search_list, ef_construction_list]):\n",
    "    M, ef_search, ef_construction = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWFlat(embedding_dim, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw\"][name_w_params(\"hnsw\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ parameters\n",
    "quantizer_list = [\n",
    "    faiss.ScalarQuantizer.QT_8bit,\n",
    "    faiss.ScalarQuantizer.QT_4bit,\n",
    "    faiss.ScalarQuantizer.QT_8bit_uniform,\n",
    "    faiss.ScalarQuantizer.QT_4bit_uniform\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"hnsw_sq\"] = dict()\n",
    "\n",
    "for params in product(*[M_list, ef_search_list, ef_construction_list, quantizer_list]):\n",
    "    M, ef_search, ef_construction, scalar_quantizer = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWSQ(embedding_dim, scalar_quantizer, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw_sq\"][name_w_params(\"hnsw_sq\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQ parameters\n",
    "M_pq_list = [4, 8, 16, 32]\n",
    "nbits_list = [6, 8, 9] # needs >= 2**nbits * 39 training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"pq\"] = dict()\n",
    "\n",
    "for params in product(*[M_pq_list, nbits_list]):\n",
    "    M_pq, nbits = params\n",
    "    assert embedding_dim % M_pq == 0\n",
    "    index = faiss.IndexPQ(embedding_dim, M_pq, nbits)\n",
    "    \n",
    "    indexes_with_params[\"pq\"][name_w_params(\"pq\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13b490b5b2643cbac3d5644f6e02091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "indexes_with_params[\"hnsw_pq\"] = dict()\n",
    "\n",
    "for params in tqdm(product(*[M_pq_list, nbits_list, M_list, ef_search_list, ef_construction_list])):\n",
    "    M_pq, nbits, M, ef_search, ef_construction = params\n",
    "    assert embedding_dim % M_pq == 0\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWPQ(embedding_dim, M_pq, M, nbits)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw_pq\"][name_w_params(\"hnsw_pq\", params)] = index\n",
    "print(len(indexes_with_params[\"hnsw_pq\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVF parameters\n",
    "quantizer_ivf_list = [faiss.IndexFlatIP, faiss.IndexFlatL2]\n",
    "nlist_list = [32, 64, 128, 256] # needs >= nlist * 39 training points\n",
    "nprobe_list = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"ivf\"] = dict()\n",
    "\n",
    "for params in product(*[quantizer_ivf_list, nlist_list, nprobe_list]):\n",
    "    quantizer, nlist, nprobe = params\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "    index = faiss.IndexIVFFlat(quantizer(embedding_dim), embedding_dim, nlist)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    quantizer = quantizer.__name__\n",
    "    indexes_with_params[\"ivf\"][name_w_params(\"ivf\", [quantizer, nlist, nprobe])] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d34dd6e466b4ed58ec81cb5892c84bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes_with_params[\"ivf_pq\"] = dict()\n",
    "\n",
    "for params in tqdm(product(*[quantizer_ivf_list, nlist_list, nprobe_list, M_pq_list, nbits_list])):\n",
    "    quantizer, nlist, nprobe, M, nbits = params\n",
    "    assert embedding_dim % M == 0\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "\n",
    "    index = faiss.IndexIVFPQ(quantizer(embedding_dim), embedding_dim, nlist, M, nbits, faiss.METRIC_L2)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    quantizer = quantizer.__name__\n",
    "    indexes_with_params[\"ivf_pq\"][name_w_params(\"ivf_pq\", [quantizer, nlist, nprobe, M, nbits])] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1731beab8446e186aa04423c0b176b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes_with_params[\"ivfHNSW\"] = dict()\n",
    "\n",
    "for params in tqdm(product(*[nlist_list, nprobe_list, M_list, ef_search_list, ef_construction_list])):\n",
    "    nlist, nprobe, M, ef_search, ef_construction = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "    quantizer = faiss.IndexHNSWFlat(embedding_dim, M)\n",
    "    quantizer.hnsw.efConstruction = ef_construction\n",
    "    quantizer.hnsw.efSearch = ef_search\n",
    "\n",
    "    index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist, faiss.METRIC_L2)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    # index.cp.min_points_per_centroid = 5\n",
    "    index.quantizer_trains_alone = 2\n",
    "\n",
    "    indexes_with_params[\"ivfHNSW\"][name_w_params(\"ivfHNSW\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16 # top-k queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(index: faiss.Index, queries, k):\n",
    "    t0 = time.time()\n",
    "    D, I = index.search(queries, k)\n",
    "    t1 = time.time()\n",
    "    return D, I, (t1 - t0) / queries.shape[0]\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum((np.power(2 * np.ones_like(scores), scores) - 1) / np.log2(np.arange(len(scores)) + 2))\n",
    "    # return np.sum(scores / np.log2(np.arange(len(scores)) + 2))\n",
    "\n",
    "\n",
    "def ndcg_with_mismatch(true_indices, true_distances, approx_indices, verbose=False):\n",
    "    # Map ground truth indices to their relevance scores\n",
    "    # true_relevance = {idx: 4 / (dist + 1) if not math.isinf(dist) else 0\n",
    "    #                   for idx, dist in zip(true_indices, true_distances)}\n",
    "    true_relevance = dict(zip(true_indices, np.exp(- 0.01 * true_distances)))\n",
    "    # print(true_relevance)\n",
    "\n",
    "    if verbose:\n",
    "        if not np.all(np.isnan(true_distances)):\n",
    "            print(\"WARNING: there is an nan distance\")\n",
    "        if not np.all(true_distances >= 0):\n",
    "            print(\"WARNING: there is a < 0 distance\")\n",
    "\n",
    "    # Create relevance scores list for the approximate indices\n",
    "    approx_relevance_scores = [\n",
    "        true_relevance.get(idx, 0) for idx in approx_indices\n",
    "    ]\n",
    "\n",
    "    # Create the ideal DCG by sorting the true relevance scores in descending order\n",
    "    ideal_relevance_scores = sorted(true_relevance.values(), reverse=True)\n",
    "    ideal_relevance_scores.extend([0]*(len(approx_relevance_scores) - len(ideal_relevance_scores)))\n",
    "\n",
    "    # Calculate DCG for both approximate and ideal relevance scores\n",
    "    dcg_approx = dcg(approx_relevance_scores)\n",
    "    dcg_ideal = dcg(ideal_relevance_scores)\n",
    "    # print(dcg_approx, dcg_ideal)\n",
    "\n",
    "    return dcg_approx / dcg_ideal if dcg_ideal > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_mean_ndcg_mismatch(ideal_index: faiss.Index, index: faiss.Index, queries: np.ndarray, k: int = 10):\n",
    "    # Get distances and indices from both indices\n",
    "    true_distances, true_indices, _ = get_top_k(ideal_index, queries, k)\n",
    "    _, approx_indices, mean_time = get_top_k(index, queries, k)\n",
    "\n",
    "    # Compute nDCG for each query with potentially mismatched sets\n",
    "    ndcg_scores = [\n",
    "        ndcg_with_mismatch(true_idx, true_dist, approx_idx)\n",
    "        for true_idx, true_dist, approx_idx in zip(true_indices, true_distances, approx_indices)\n",
    "    ]\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "    return mean_ndcg, mean_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disk_usage(index: faiss.Index, index_name: str):\n",
    "    file_name = \"../data/index/\" + index_name + \".index\"\n",
    "    faiss.write_index(index, file_name)\n",
    "\n",
    "    index_size = os.path.getsize(file_name)\n",
    "    os.remove(file_name)\n",
    "\n",
    "    index_size_mb = index_size / (1024 * 1024)\n",
    "\n",
    "    return index_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_index(index: faiss.Index, data):\n",
    "    t0 = time.time()\n",
    "    index.train(data)\n",
    "    t1 = time.time()\n",
    "\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(index: faiss.Index, data):\n",
    "    assert index.is_trained\n",
    "    t0 = time.time()\n",
    "    index.add(data)\n",
    "    t1 = time.time()\n",
    "\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_down(a: float):\n",
    "    return math.floor(a * 100)/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np_train = chunks_base_emb[:20000]\n",
    "embeddings_np_test = questions_base_emb[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b3f436a90941b89647428246be9ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9f15bf38b643bfa00fb42c5cf5c0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de80e96a45014cd8bd12067a50e9eb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a63dea4897e41b7a4d828b8cd7dd9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results: list[list] = list()\n",
    "\n",
    "for group_name in indexes_with_params.keys():\n",
    "    if os.path.exists(\"../data/index_res_base/\" + group_name + \".csv\"):\n",
    "        continue\n",
    "\n",
    "    group_results: list[list] = list()\n",
    "    group_best: list = [\"\", -1, -1, -1, -1]\n",
    "\n",
    "    list_indexes = list(indexes_with_params[group_name].keys())\n",
    "    if len(list_indexes) > 50:\n",
    "        list_indexes = np.sort(np.random.choice(list_indexes, max(len(list_indexes)//5, 50), replace=False))\n",
    "    \n",
    "    pbar = tqdm(list_indexes)\n",
    "    for name in pbar:\n",
    "        index = indexes_with_params[group_name][name]\n",
    "        pbar.set_description(name)\n",
    "        train_time = -1 if index.is_trained else train_index(index, embeddings_np_train)*1000\n",
    "        assert index.is_trained\n",
    "        construct_time = construct_index(index, embeddings_np_train)*1000\n",
    "        assert index.ntotal > 0\n",
    "        flat_index_local = faiss.IndexFlatL2(embedding_dim)\n",
    "        flat_index_local.add(embeddings_np_train)\n",
    "        mean_ndcg, mean_time = calculate_mean_ndcg_mismatch(flat_index_local, index, embeddings_np_test, k)\n",
    "        index_size_mb = calculate_disk_usage(index, name)\n",
    "        if mean_ndcg > 1:\n",
    "            print(mean_ndcg)\n",
    "            assert 0\n",
    "        index.reset()\n",
    "\n",
    "        res = [\n",
    "            name,\n",
    "            round_down(mean_ndcg),\n",
    "            round_down(index_size_mb),\n",
    "            round_down(mean_time*1000),\n",
    "            round_down(train_time),\n",
    "            round_down(construct_time)\n",
    "        ]\n",
    "        group_results.append(res)\n",
    "        if res[1] > group_best[1]:\n",
    "            group_best = res\n",
    "            pbar.set_postfix({\"NDCG\": res[1], \"index\": name})\n",
    "\n",
    "    df = pd.DataFrame(group_results, columns=[\"index\", \"NDCG\", \"size_mb\", \"mean_time_query_ms\", \"time_train_ms\", \"time_construct_ms\"])\n",
    "    df.to_csv(\"../data/index_res_base/\" + group_name + \".csv\")\n",
    "    results.extend(group_results)\n",
    "    pbar.set_description(group_name)\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=[\"index\", \"NDCG\", \"size_mb\", \"mean_time_query_ms\", \"time_train_ms\", \"time_construct_ms\"])\n",
    "res_df.to_csv(\"../data/index_res_base/all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
