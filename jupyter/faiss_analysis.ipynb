{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"../data/jina.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tag_embeddings(filename: str) -> tuple[list[str], list[list]]:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    tags = [item['tag'] for item in data]\n",
    "    embeddings = [item['embedding'] for item in data]\n",
    "\n",
    "    return tags, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_w_params(name: str, params: list[object]):\n",
    "    return \"_\".join([name] + [p.__name__ if isinstance(p, faiss.Index) else str(p) for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, embeddings = load_tag_embeddings(embeddings_path)\n",
    "embeddings_np = np.array(embeddings, dtype='float32')\n",
    "embedding_dim = embeddings_np.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "test_size = 1000\n",
    "test_ids = np.random.choice(np.arange(0, embeddings_np.shape[0]), test_size)\n",
    "train_ids = np.delete(np.arange(0, embeddings_np.shape[0]), test_ids)\n",
    "embeddings_np_test = embeddings_np.take(test_ids, 0)\n",
    "embeddings_np_train = embeddings_np.take(train_ids, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params = {\"flat\": {\"flat\" : flat_index}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW paremeters\n",
    "M_list = [8, 16, 32, 64, 128]  # number of connections each vertex will have\n",
    "ef_search_list = [8, 16, 32, 64, 128]  # depth of layers explored during search\n",
    "ef_construction_list = [8, 16, 32, 64, 128]  # depth of layers explored during index construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"hnsw\"] = dict()\n",
    "\n",
    "for params in product(*[M_list, ef_search_list, ef_construction_list]):\n",
    "    M, ef_search, ef_construction = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWFlat(embedding_dim, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw\"][name_w_params(\"hnsw\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQ parameters\n",
    "quantizer_list = [\n",
    "    faiss.ScalarQuantizer.QT_8bit,\n",
    "    faiss.ScalarQuantizer.QT_4bit,\n",
    "    faiss.ScalarQuantizer.QT_8bit_uniform,\n",
    "    faiss.ScalarQuantizer.QT_4bit_uniform\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"hnsw_sq\"] = dict()\n",
    "\n",
    "for params in product(*[M_list, ef_search_list, ef_construction_list, quantizer_list]):\n",
    "    M, ef_search, ef_construction, scalar_quantizer = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWSQ(embedding_dim, scalar_quantizer, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw_sq\"][name_w_params(\"hnsw_sq\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQ parameters\n",
    "M_pq_list = [4, 8, 16]\n",
    "nbits_list = [3, 4, 5, 6, 7] # needs >= 2**nbits * 39 training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"pq\"] = dict()\n",
    "\n",
    "for params in product(*[M_pq_list, nbits_list]):\n",
    "    M_pq, nbits = params\n",
    "    assert embedding_dim % M_pq == 0\n",
    "    index = faiss.IndexPQ(embedding_dim, M_pq, nbits)\n",
    "    \n",
    "    indexes_with_params[\"pq\"][name_w_params(\"pq\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n"
     ]
    }
   ],
   "source": [
    "indexes_with_params[\"hnsw_pq\"] = dict()\n",
    "\n",
    "for params in product(*[M_pq_list, nbits_list, M_list, ef_search_list, ef_construction_list]):\n",
    "    M_pq, nbits, M, ef_search, ef_construction = params\n",
    "    assert embedding_dim % M_pq == 0\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    index = faiss.IndexHNSWPQ(embedding_dim, M_pq, M, nbits)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    index.hnsw.efSearch = ef_search\n",
    "\n",
    "    indexes_with_params[\"hnsw_pq\"][name_w_params(\"hnsw_pq\", params)] = index\n",
    "print(len(indexes_with_params[\"hnsw_pq\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVF parameters\n",
    "quantizer_ivf_list = [faiss.IndexFlatIP, faiss.IndexFlatL2]\n",
    "nlist_list = [8, 16, 32, 64, 128] # needs >= nlist * 39 training points\n",
    "nprobe_list = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_params[\"ivf\"] = dict()\n",
    "\n",
    "for params in product(*[quantizer_ivf_list, nlist_list, nprobe_list]):\n",
    "    quantizer, nlist, nprobe = params\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "    index = faiss.IndexIVFFlat(quantizer(embedding_dim), embedding_dim, nlist)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    quantizer = quantizer.__name__\n",
    "    indexes_with_params[\"ivf\"][name_w_params(\"ivf\", [quantizer, nlist, nprobe])] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2279b191fca24f3ca11632ebc3d02306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes_with_params[\"ivf_pq\"] = dict()\n",
    "\n",
    "for params in tqdm(product(*[quantizer_ivf_list, nlist_list, nprobe_list, M_pq_list, nbits_list])):\n",
    "    quantizer, nlist, nprobe, M, nbits = params\n",
    "    assert embedding_dim % M == 0\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "\n",
    "    index = faiss.IndexIVFPQ(quantizer(embedding_dim), embedding_dim, nlist, M, nbits, faiss.METRIC_L2)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    quantizer = quantizer.__name__\n",
    "    indexes_with_params[\"ivf_pq\"][name_w_params(\"ivf_pq\", [quantizer, nlist, nprobe, M, nbits])] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289df77c766843a6bb9743fe2512b24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexes_with_params[\"ivfHNSW\"] = dict()\n",
    "\n",
    "for params in tqdm(product(*[nlist_list, nprobe_list, M_list, ef_search_list, ef_construction_list])):\n",
    "    nlist, nprobe, M, ef_search, ef_construction = params\n",
    "    if ef_search > ef_construction:\n",
    "        continue\n",
    "    if nprobe > nlist:\n",
    "        continue\n",
    "    quantizer = faiss.IndexHNSWFlat(embedding_dim, M)\n",
    "    quantizer.hnsw.efConstruction = ef_construction\n",
    "    quantizer.hnsw.efSearch = ef_search\n",
    "\n",
    "    index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist, faiss.METRIC_L2)\n",
    "    index.nprobe = nprobe\n",
    "\n",
    "    # index.cp.min_points_per_centroid = 5\n",
    "    index.quantizer_trains_alone = 2\n",
    "\n",
    "    indexes_with_params[\"ivfHNSW\"][name_w_params(\"ivfHNSW\", params)] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32 # top-k queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(index: faiss.Index, queries, k):\n",
    "    t0 = time.time()\n",
    "    D, I = index.search(queries, k)\n",
    "    t1 = time.time()\n",
    "    return D, I, (t1 - t0) / queries.shape[0]\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum((np.power(2 * np.ones_like(scores), scores) - 1) / np.log2(np.arange(len(scores)) + 2))\n",
    "    # return np.sum(scores / np.log2(np.arange(len(scores)) + 2))\n",
    "\n",
    "\n",
    "def ndcg_with_mismatch(true_indices, true_distances, approx_indices, verbose=False):\n",
    "    # Map ground truth indices to their relevance scores\n",
    "    true_relevance = {idx: 4 / (dist + 1) if not math.isinf(dist) else 0\n",
    "                      for idx, dist in zip(true_indices, true_distances)}\n",
    "    if verbose:\n",
    "        if not np.all(np.isnan(true_distances)):\n",
    "            print(\"WARNING: there is an nan distance\")\n",
    "        if not np.all(true_distances >= 0):\n",
    "            print(\"WARNING: there is a < 0 distance\")\n",
    "\n",
    "    # Create relevance scores list for the approximate indices\n",
    "    approx_relevance_scores = [\n",
    "        true_relevance.get(idx, 0) for idx in approx_indices\n",
    "    ]\n",
    "\n",
    "    # Create the ideal DCG by sorting the true relevance scores in descending order\n",
    "    ideal_relevance_scores = sorted(true_relevance.values(), reverse=True)\n",
    "    ideal_relevance_scores.extend([0]*(len(approx_relevance_scores) - len(ideal_relevance_scores)))\n",
    "\n",
    "    # Calculate DCG for both approximate and ideal relevance scores\n",
    "    dcg_approx = dcg(approx_relevance_scores)\n",
    "    dcg_ideal = dcg(ideal_relevance_scores)\n",
    "    # print(dcg_approx, dcg_ideal)\n",
    "\n",
    "    return dcg_approx / dcg_ideal if dcg_ideal > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_mean_ndcg_mismatch(ideal_index: faiss.Index, index: faiss.Index, queries: np.ndarray, k: int = 10):\n",
    "    # Get distances and indices from both indices\n",
    "    true_distances, true_indices, _ = get_top_k(ideal_index, queries, k)\n",
    "    _, approx_indices, mean_time = get_top_k(index, queries, k)\n",
    "\n",
    "    # Compute nDCG for each query with potentially mismatched sets\n",
    "    ndcg_scores = [\n",
    "        ndcg_with_mismatch(true_idx, true_dist, approx_idx)\n",
    "        for true_idx, true_dist, approx_idx in zip(true_indices, true_distances, approx_indices)\n",
    "    ]\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "    return mean_ndcg, mean_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disk_usage(index: faiss.Index, index_name: str):\n",
    "    file_name = \"../data/index/\" + index_name + \".index\"\n",
    "    faiss.write_index(index, file_name)\n",
    "\n",
    "    index_size = os.path.getsize(file_name)\n",
    "    os.remove(file_name)\n",
    "\n",
    "    index_size_mb = index_size / (1024 * 1024)\n",
    "\n",
    "    return index_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_index(index: faiss.Index, data):\n",
    "    t0 = time.time()\n",
    "    index.train(data)\n",
    "    t1 = time.time()\n",
    "\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(index: faiss.Index, data):\n",
    "    assert index.is_trained\n",
    "    t0 = time.time()\n",
    "    index.add(data)\n",
    "    t1 = time.time()\n",
    "\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_down(a: float):\n",
    "    return math.floor(a * 100)/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ef74b7f66d4352b632476320d09e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211e61db08ae4a61b98ae49354ef7314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792ac11e6b0b480da482943ee736155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7163a082d824ddaa0f3474e2a34d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd1fd0ab4c4594962e3044ff271949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b550c2c9674cd1b06c36a026db72c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d289699629c14091a6a693dc7342b790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191970d8db5745788aeb706112b93950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results: list[list] = list()\n",
    "\n",
    "for group_name in indexes_with_params.keys():\n",
    "    group_results: list[list] = list()\n",
    "    group_best: list = [\"\", -1, -1, -1, -1]\n",
    "\n",
    "    list_indexes = list(indexes_with_params[group_name].keys())\n",
    "    if len(list_indexes) > 100:\n",
    "        list_indexes = np.sort(np.random.choice(list_indexes, max(len(list_indexes)//5, 100), replace=False))\n",
    "    \n",
    "    pbar = tqdm(list_indexes)\n",
    "    for name in pbar:\n",
    "        index = indexes_with_params[group_name][name]\n",
    "        pbar.set_description(name)\n",
    "        train_time = -1 if index.is_trained else train_index(index, embeddings_np_train)*1000\n",
    "        assert index.is_trained\n",
    "        construct_time = construct_index(index, embeddings_np_train)*1000\n",
    "        assert index.ntotal > 0\n",
    "        flat_index_local = faiss.IndexFlatL2(embedding_dim)\n",
    "        flat_index_local.add(embeddings_np_train)\n",
    "        mean_ndcg, mean_time = calculate_mean_ndcg_mismatch(flat_index_local, index, embeddings_np_test, k)\n",
    "        index_size_mb = calculate_disk_usage(index, name)\n",
    "        if mean_ndcg > 1:\n",
    "            print(mean_ndcg)\n",
    "            assert 0\n",
    "        index.reset()\n",
    "\n",
    "        res = [\n",
    "            name,\n",
    "            round_down(mean_ndcg),\n",
    "            round_down(index_size_mb),\n",
    "            round_down(mean_time*1000),\n",
    "            round_down(train_time),\n",
    "            round_down(construct_time)\n",
    "        ]\n",
    "        group_results.append(res)\n",
    "        if res[1] > group_best[1]:\n",
    "            group_best = res\n",
    "            pbar.set_postfix({\"NDCG\": res[1], \"index\": name})\n",
    "\n",
    "    df = pd.DataFrame(group_results, columns=[\"index\", \"NDCG\", \"size_mb\", \"mean_time_query_ms\", \"time_train_ms\", \"time_construct_ms\"])\n",
    "    df.to_csv(\"../data/index_res/\" + group_name + \".csv\")\n",
    "    results.extend(group_results)\n",
    "    pbar.set_description(group_name)\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=[\"index\", \"NDCG\", \"size_mb\", \"mean_time_query_ms\", \"time_train_ms\", \"time_construct_ms\"])\n",
    "res_df.to_csv(\"../data/index_res/all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
